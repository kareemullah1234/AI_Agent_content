{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWrb2JUvVgNJGT4uwCaLpH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kareemullah1234/AI_Agent_content/blob/main/Agent_6_streaming_persist_sql_remebers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U \\\n",
        "  langchain-core \\\n",
        "  langchain \\\n",
        "  langchain-groq \\\n",
        "  langchain-tavily \\\n",
        "  langgraph \\\n",
        "  langgraph-checkpoint \\\n",
        "  aiosqlite \\\n",
        "  sqlite-utils\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1NM-OiOJ61W",
        "outputId": "4dac83a6-6111-41cc-9075-d799732ba9af"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m153.2/153.2 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.2/68.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m131.4/131.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph-checkpoint-sqlite  #important\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzLpc4OuMnku",
        "outputId": "17f5ee05-7bef-4972-dc9a-481aabf28f4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph-checkpoint-sqlite\n",
            "  Downloading langgraph_checkpoint_sqlite-2.0.11-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: aiosqlite>=0.20 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint-sqlite) (0.21.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.21 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint-sqlite) (2.1.1)\n",
            "Collecting sqlite-vec>=0.1.6 (from langgraph-checkpoint-sqlite)\n",
            "  Downloading sqlite_vec-0.1.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux1_x86_64.whl.metadata (198 bytes)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.11/dist-packages (from aiosqlite>=0.20->langgraph-checkpoint-sqlite) (4.14.1)\n",
            "Requirement already satisfied: langchain-core>=0.2.38 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (0.3.74)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (1.10.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (0.4.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (6.0.2)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (2.11.7)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (2.32.3)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (0.4.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (1.3.1)\n",
            "Downloading langgraph_checkpoint_sqlite-2.0.11-py3-none-any.whl (31 kB)\n",
            "Downloading sqlite_vec-0.1.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux1_x86_64.whl (151 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sqlite-vec, langgraph-checkpoint-sqlite\n",
            "Successfully installed langgraph-checkpoint-sqlite-2.0.11 sqlite-vec-0.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "\n",
        "conn = sqlite3.connect(\"memory.db\", check_same_thread=False)\n",
        "memory = SqliteSaver(conn)\n",
        "print(\"âœ… SqliteSaver imported successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcSDbX3oMnoI",
        "outputId": "8d777f9c-dd9e-4dd5-860a-8a9306a7921a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… SqliteSaver imported successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# # Lesson 4: Persistence and Streaming (Modified for Groq)\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "\n",
        "# Install required packages (Run this cell first, then RESTART the runtime)\n",
        "# !pip install -q -U langchain-core langchain langchain-groq langchain-tavily langgraph langgraph-checkpoint aiosqlite sqlite-utils\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ðŸ”‘ Hardcode API keys\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "# In[3]:\n",
        "\n",
        "\n",
        "# Import necessary modules\n",
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, Annotated\n",
        "import operator\n",
        "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
        "\n",
        "# ðŸ” Use ChatGroq instead of ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# ðŸ” Use the updated Tavily tool\n",
        "from langchain_tavily import TavilySearch\n",
        "\n",
        "\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "# Initialize the Tavily tool\n",
        "tool = TavilySearch(max_results=2)\n",
        "\n",
        "\n",
        "# In[5]:\n",
        "\n",
        "\n",
        "# Define the agent state\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], operator.add]\n",
        "\n",
        "\n",
        "\n",
        "# Define the Agent class\n",
        "class Agent:\n",
        "    def __init__(self, model, tools, checkpointer, system=\"\"):\n",
        "        self.system = system\n",
        "        graph = StateGraph(AgentState)\n",
        "        graph.add_node(\"llm\", self.call_llm) # Renamed for clarity\n",
        "        graph.add_node(\"action\", self.take_action)\n",
        "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
        "        graph.add_edge(\"action\", \"llm\")\n",
        "        graph.set_entry_point(\"llm\")\n",
        "        self.graph = graph.compile(checkpointer=checkpointer)\n",
        "        self.tools = {t.name: t for t in tools}\n",
        "        self.model = model.bind_tools(tools)\n",
        "\n",
        "    # ðŸ” Updated to call Groq model\n",
        "    def call_llm(self, state: AgentState):\n",
        "        messages = state['messages']\n",
        "        if self.system:\n",
        "            messages = [SystemMessage(content=self.system)] + messages\n",
        "        message = self.model.invoke(messages)\n",
        "        return {'messages': [message]}\n",
        "\n",
        "    def exists_action(self, state: AgentState):\n",
        "        result = state['messages'][-1]\n",
        "        return len(result.tool_calls) > 0\n",
        "\n",
        "    def take_action(self, state: AgentState):\n",
        "        tool_calls = state['messages'][-1].tool_calls\n",
        "        results = []\n",
        "        for t in tool_calls:\n",
        "            print(f\"Calling: {t}\")\n",
        "            result = self.tools[t['name']].invoke(t['args'])\n",
        "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
        "        print(\"Back to the model!\")\n",
        "        return {'messages': results}\n",
        "\n",
        "\n",
        "# In[8]:\n",
        "\n",
        "\n",
        "# ðŸš€ Initialize the Groq model\n",
        "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
        "You are allowed to make multiple calls (either together or in sequence). \\\n",
        "Only look up information when you are sure of what you want. \\\n",
        "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
        "\"\"\"\n",
        "# Use a Groq model that supports tool calling\n",
        "model = ChatGroq(model=\"llama3-8b-8192\", temperature=0.3, max_tokens=512)\n",
        "\n",
        "\n",
        "# In[9]:\n",
        "\n",
        "\n",
        "# Create the agent instance with Groq model and SQLite persistence\n",
        "abot = Agent(model, [tool], system=prompt, checkpointer=memory)\n"
      ],
      "metadata": {
        "id": "nkVJTwToNO9X"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# In[10]:\n",
        "\n",
        "\n",
        "# ðŸ§ª Test the agent\n",
        "messages = [HumanMessage(content=\"What is the weather in sf?\")]\n",
        "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "# Stream the response\n",
        "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
        "    for v in event.values():\n",
        "        print(v['messages'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me7f4avfEGf1",
        "outputId": "04a4bfee-aee5-4b53-c215-c33f53cb37f6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '7e0j5ytp8', 'function': {'arguments': '{\"query\":\"weather in sf\",\"search_depth\":\"basic\",\"topic\":\"general\"}', 'name': 'tavily_search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 96, 'prompt_tokens': 2807, 'total_tokens': 2903, 'completion_time': 0.080663435, 'prompt_time': 0.316417031, 'queue_time': 0.003841487, 'total_time': 0.397080466}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_343314801a', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--3bbe1b24-2141-4fa2-a160-01c9ffd68686-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'weather in sf', 'search_depth': 'basic', 'topic': 'general'}, 'id': '7e0j5ytp8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2807, 'output_tokens': 96, 'total_tokens': 2903})]\n",
            "Calling: {'name': 'tavily_search', 'args': {'query': 'weather in sf', 'search_depth': 'basic', 'topic': 'general'}, 'id': '7e0j5ytp8', 'type': 'tool_call'}\n",
            "Back to the model!\n",
            "[ToolMessage(content='{\\'query\\': \\'weather in sf\\', \\'follow_up_questions\\': None, \\'answer\\': None, \\'images\\': [], \\'results\\': [{\\'title\\': \\'Weather in San Francisco\\', \\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.775, \\'lon\\': -122.4183, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1755367950, \\'localtime\\': \\'2025-08-16 11:12\\'}, \\'current\\': {\\'last_updated_epoch\\': 1755367200, \\'last_updated\\': \\'2025-08-16 11:00\\', \\'temp_c\\': 18.9, \\'temp_f\\': 66.0, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Overcast\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/122.png\\', \\'code\\': 1009}, \\'wind_mph\\': 8.9, \\'wind_kph\\': 14.4, \\'wind_degree\\': 236, \\'wind_dir\\': \\'WSW\\', \\'pressure_mb\\': 1017.0, \\'pressure_in\\': 30.02, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 81, \\'cloud\\': 100, \\'feelslike_c\\': 18.9, \\'feelslike_f\\': 66.0, \\'windchill_c\\': 15.9, \\'windchill_f\\': 60.6, \\'heatindex_c\\': 15.9, \\'heatindex_f\\': 60.6, \\'dewpoint_c\\': 15.2, \\'dewpoint_f\\': 59.4, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 3.2, \\'gust_mph\\': 13.2, \\'gust_kph\\': 21.3}}\", \\'score\\': 0.9928174, \\'raw_content\\': None}, {\\'url\\': \\'https://weathershogun.com/weather/usa/ca/san-francisco/480/august/2025-08-16\\', \\'title\\': \\'Saturday, August 16, 2025. San Francisco, CA - Weather Forecast\\', \\'content\\': \"Saturday, August 16, 2025. San Francisco, CA - Weather Forecast  San Francisco, CA Image 1: WeatherShogun.com HomeContactBrowse StatesPrivacy PolicyTerms and Conditions Â°F)Â°C) TodayTomorrowHourly7 days30 daysAugust San Francisco, California Weather:  Saturday, August 16, 2025 Day 70Â° Night 57Â° Precipitation 84 % Wind 27 mph UV Index (0 - 11+)11 Sunday *   Hourly *   Today *   Tomorrow *   7 days *   30 days Weather Forecast History Last Year\\'s Weather on This Day (August 16, 2024) ### Day 70Â° ### Night 55Â° #### Wind 20 mph #### UV Index #### Precipitation Please note that while we strive for accuracy, the information provided may not always be correct. Use at your own risk. Â© Copyright by WeatherShogun.com\", \\'score\\': 0.95763195, \\'raw_content\\': None}], \\'response_time\\': 1.63, \\'request_id\\': \\'a0ec9d3a-0960-43f4-9c95-92019cda0f19\\'}', name='tavily_search', tool_call_id='7e0j5ytp8')]\n",
            "[AIMessage(content='Based on the results from the tool call, the current weather in San Francisco is overcast with a temperature of 66Â°F (18.9Â°C) and a wind speed of 14.4 km/h (8.9 mph).', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 3659, 'total_tokens': 3708, 'completion_time': 0.041129042, 'prompt_time': 0.407273972, 'queue_time': 0.004325149, 'total_time': 0.448403014}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_343314801a', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--a3566f63-ade1-4184-95ad-91319d3735a0-0', usage_metadata={'input_tokens': 3659, 'output_tokens': 49, 'total_tokens': 3708})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# In[11]:\n",
        "\n",
        "\n",
        "# Ask a follow-up question (persistence should remember the conversation)\n",
        "messages = [HumanMessage(content=\"What about in la?\")]\n",
        "thread = {\"configurable\": {\"thread_id\": \"1\"}} # Same thread ID\n",
        "\n",
        "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
        "    for v in event.values():\n",
        "        print(v['messages'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeeNYCWfEOH0",
        "outputId": "53b77bc8-4a82-4938-c5f1-db51649054db"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'avb7kn7sn', 'function': {'arguments': '{\"query\":\"weather in la\",\"search_depth\":\"basic\",\"topic\":\"general\"}', 'name': 'tavily_search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 3722, 'total_tokens': 3768, 'completion_time': 0.038490924, 'prompt_time': 0.413777928, 'queue_time': 0.004293296, 'total_time': 0.452268852}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_0fb809dba3', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--bd839012-54b3-47a0-91e6-4bff381565a9-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'weather in la', 'search_depth': 'basic', 'topic': 'general'}, 'id': 'avb7kn7sn', 'type': 'tool_call'}], usage_metadata={'input_tokens': 3722, 'output_tokens': 46, 'total_tokens': 3768})]\n",
            "Calling: {'name': 'tavily_search', 'args': {'query': 'weather in la', 'search_depth': 'basic', 'topic': 'general'}, 'id': 'avb7kn7sn', 'type': 'tool_call'}\n",
            "Back to the model!\n",
            "[ToolMessage(content='{\\'query\\': \\'weather in la\\', \\'follow_up_questions\\': None, \\'answer\\': None, \\'images\\': [], \\'results\\': [{\\'title\\': \\'Weather in Los Angeles\\', \\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"{\\'location\\': {\\'name\\': \\'Los Angeles\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 34.0522, \\'lon\\': -118.2428, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1755367349, \\'localtime\\': \\'2025-08-16 11:02\\'}, \\'current\\': {\\'last_updated_epoch\\': 1755367200, \\'last_updated\\': \\'2025-08-16 11:00\\', \\'temp_c\\': 20.3, \\'temp_f\\': 68.5, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Overcast\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/122.png\\', \\'code\\': 1009}, \\'wind_mph\\': 6.5, \\'wind_kph\\': 10.4, \\'wind_degree\\': 204, \\'wind_dir\\': \\'SSW\\', \\'pressure_mb\\': 1018.0, \\'pressure_in\\': 30.05, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 76, \\'cloud\\': 100, \\'feelslike_c\\': 20.3, \\'feelslike_f\\': 68.5, \\'windchill_c\\': 22.8, \\'windchill_f\\': 73.0, \\'heatindex_c\\': 24.1, \\'heatindex_f\\': 75.3, \\'dewpoint_c\\': 16.7, \\'dewpoint_f\\': 62.0, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 6.1, \\'gust_mph\\': 7.5, \\'gust_kph\\': 12.0}}\", \\'score\\': 0.9801697, \\'raw_content\\': None}, {\\'url\\': \\'https://www.weather25.com/north-america/usa/california/los-angeles?page=month&month=August\\', \\'title\\': \\'Los Angeles weather in August 2025 | Weather25.com\\', \\'content\\': \"Los Angeles weather in August 2025 | Weather25.com Los Angeles Image 3: weather in United States Los Angeles weather in August 2025 The average weather in Los Angeles in August The weather in Los Angeles in August is hot. There shouldn\\'t be any rainy days in in Los Angeles during August. Temperatures in Los Angeles in August Weather in Los Angeles in August - FAQ The average temperature in Los Angeles in August is 69/87Â° F. On average, there are 0 rainy days in Los Angeles during August. The weather in Los Angeles in August is good. On average, there are 0 snowy days in Los Angeles in August. More about the weather in Los Angeles\", \\'score\\': 0.8969848, \\'raw_content\\': None}], \\'response_time\\': 1.4, \\'request_id\\': \\'94f0470d-a28b-4281-bb06-2c7fe263be26\\'}', name='tavily_search', tool_call_id='avb7kn7sn')]\n",
            "[AIMessage(content='Based on the results from the tool call, the current weather in Los Angeles is overcast with a temperature of 68.5Â°F (20.3Â°C) and a wind speed of 10.4 km/h (6.5 mph).', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 4509, 'total_tokens': 4560, 'completion_time': 0.042314838, 'prompt_time': 0.500957713, 'queue_time': 0.004769837, 'total_time': 0.543272551}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_0fb809dba3', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--5d3f444b-9749-42ef-8a01-a031a49a9a86-0', usage_metadata={'input_tokens': 4509, 'output_tokens': 51, 'total_tokens': 4560})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In[12]:\n",
        "\n",
        "\n",
        "# Ask another question comparing previous results\n",
        "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
        "thread = {\"configurable\": {\"thread_id\": \"1\"}} # Same thread ID\n",
        "\n",
        "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
        "    for v in event.values():\n",
        "        print(v['messages'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ty24nNqEVBv",
        "outputId": "4979cdc9-9219-40f0-e160-d353cdb13995"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AIMessage(content='According to the results, San Francisco (66Â°F/18.9Â°C) is slightly cooler than Los Angeles (68.5Â°F/20.3Â°C).', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 4574, 'total_tokens': 4608, 'completion_time': 0.028119841, 'prompt_time': 0.503627547, 'queue_time': 0.005475129, 'total_time': 0.531747388}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_0fb809dba3', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--0da2c81d-8737-4fbf-99a7-cb68dfd149c9-0', usage_metadata={'input_tokens': 4574, 'output_tokens': 34, 'total_tokens': 4608})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In[13]:\n",
        "\n",
        "\n",
        "# Start a new conversation thread\n",
        "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
        "thread = {\"configurable\": {\"thread_id\": \"2\"}} # New thread ID\n",
        "\n",
        "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
        "    for v in event.values():\n",
        "        print(v['messages'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ham1C6t1Ea94",
        "outputId": "68e3869d-75ef-48ac-caae-e7c07c3110e7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AIMessage(content=\"I'm happy to help! However, I need more information to provide a accurate answer. Can you please specify what you are referring to? Are you asking about the temperature of a specific location, a person, or something else?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 2805, 'total_tokens': 2852, 'completion_time': 0.039573516, 'prompt_time': 0.31453707, 'queue_time': 0.003754728, 'total_time': 0.354110586}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_343314801a', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--65e0d591-3efc-4900-a5f3-0120058e0d05-0', usage_metadata={'input_tokens': 2805, 'output_tokens': 47, 'total_tokens': 2852})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ## Streaming tokens (Example - requires async environment)\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# # For token streaming, you would typically use async\n",
        "# from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver # Ensure aiosqlite is installed\n",
        "# import asyncio\n",
        "\n",
        "# # Set up async memory\n",
        "# # async_memory = AsyncSqliteSaver.from_conn_string(\"memory.db\") # Or \":memory:\"\n",
        "# # abot_stream = Agent(model, [tool], system=prompt, checkpointer=async_memory)\n",
        "\n",
        "# # Example async streaming function (requires running in an async context)\n",
        "# # async def stream_response():\n",
        "# #     messages = [HumanMessage(content=\"What is the weather in SF?\")]\n",
        "# #     thread = {\"configurable\": {\"thread_id\": \"4\"}}\n",
        "# #     async for event in abot_stream.graph.astream_events({\"messages\": messages}, thread, version=\"v1\"):\n",
        "# #         kind = event[\"event\"]\n",
        "# #         if kind == \"on_chat_model_stream\":\n",
        "# #             content = event[\"data\"][\"chunk\"].content\n",
        "# #             if content:\n",
        "# #                 print(content, end=\"|\")\n",
        "# # await stream_response() # This line would be run in an async environment\n"
      ],
      "metadata": {
        "id": "JZYJS_lNDt-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DYp54NQgNPAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JoB3XorwNPLF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}