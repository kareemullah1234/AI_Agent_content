{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQXE9wrfGb4KphamXKTKw7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kareemullah1234/AI_Agent_content/blob/main/Agent_7_Paper_MCP_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 1. Install Dependencies\n",
        "# =============================\n",
        "!pip install groq arxiv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZV2r2f39nhW",
        "outputId": "3c2d9601-018d-42b1-c9cc-a5b5a0c85f50"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.31.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting arxiv\n",
            "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.14.1)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.11/dist-packages (from arxiv) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2.5.0)\n",
            "Downloading groq-0.31.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.4/131.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=ef43f9ab9fec88b15ad556bfe2e7a6560171f61cad23342561bc0fa5a931dc7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv, groq\n",
            "Successfully installed arxiv-2.2.0 feedparser-6.0.11 groq-0.31.0 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BEHVTGIR9ihN"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# =============================\n",
        "# 2. Imports & Config\n",
        "# =============================\n",
        "import os\n",
        "import arxiv\n",
        "import json\n",
        "from typing import List\n",
        "from groq import Groq\n",
        "\n",
        "# --- Hardcoded API key ---\n",
        "os.environ[\"GROQ_API_KEY\"] = \"\"\n",
        "\n",
        "# Initialize Groq client\n",
        "client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "MODEL = \"llama3-70b-8192\"   # You can also try \"mixtral-8x7b-32768\"\n",
        "\n",
        "# Directory to store papers\n",
        "PAPER_DIR = \"papers\"\n",
        "os.makedirs(PAPER_DIR, exist_ok=True)\n",
        "\n",
        "# =============================\n",
        "# 3. Paper Search Function\n",
        "# =============================\n",
        "def search_papers(topic: str, max_results: int = 5) -> List[str]:\n",
        "    client_arxiv = arxiv.Client()\n",
        "    search = arxiv.Search(\n",
        "        query=topic,\n",
        "        max_results=max_results,\n",
        "        sort_by=arxiv.SortCriterion.Relevance\n",
        "    )\n",
        "\n",
        "    papers = client_arxiv.results(search)\n",
        "    path = os.path.join(PAPER_DIR, topic.lower().replace(\" \", \"_\"))\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "    file_path = os.path.join(path, \"papers_info.json\")\n",
        "    try:\n",
        "        with open(file_path, \"r\") as json_file:\n",
        "            papers_info = json.load(json_file)\n",
        "    except (FileNotFoundError, json.JSONDecodeError):\n",
        "        papers_info = {}\n",
        "\n",
        "    paper_ids = []\n",
        "    for paper in papers:\n",
        "        paper_ids.append(paper.get_short_id())\n",
        "        paper_info = {\n",
        "            \"title\": paper.title,\n",
        "            \"authors\": [author.name for author in paper.authors],\n",
        "            \"summary\": paper.summary,\n",
        "            \"pdf_url\": paper.pdf_url,\n",
        "            \"published\": str(paper.published.date())\n",
        "        }\n",
        "        papers_info[paper.get_short_id()] = paper_info\n",
        "\n",
        "    with open(file_path, \"w\") as json_file:\n",
        "        json.dump(papers_info, json_file, indent=2)\n",
        "\n",
        "    print(f\"Results are saved in: {file_path}\")\n",
        "    return paper_ids\n",
        "\n",
        "# =============================\n",
        "# 4. Extract Paper Info\n",
        "# =============================\n",
        "def extract_info(paper_id: str) -> str:\n",
        "    for item in os.listdir(PAPER_DIR):\n",
        "        item_path = os.path.join(PAPER_DIR, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            file_path = os.path.join(item_path, \"papers_info.json\")\n",
        "            if os.path.isfile(file_path):\n",
        "                try:\n",
        "                    with open(file_path, \"r\") as json_file:\n",
        "                        papers_info = json.load(json_file)\n",
        "                        if paper_id in papers_info:\n",
        "                            return json.dumps(papers_info[paper_id], indent=2)\n",
        "                except (FileNotFoundError, json.JSONDecodeError):\n",
        "                    continue\n",
        "    return f\"There’s no saved information related to paper {paper_id}.\"\n",
        "\n",
        "# =============================\n",
        "# 5. Process Query with Groq\n",
        "# =============================\n",
        "def process_query(query: str):\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[{\"role\": \"user\", \"content\": query}],\n",
        "        temperature=0.7,\n",
        "        max_tokens=1024,\n",
        "    )\n",
        "    reply = response.choices[0].message.content\n",
        "    print(reply)\n",
        "    return reply\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 6. Example Run\n",
        "# =============================\n",
        "# Search for papers\n",
        "ids = search_papers(\"machine learning\", max_results=3)\n",
        "print(\"Paper IDs:\", ids)\n",
        "\n",
        "# Extract info\n",
        "print(extract_info(ids[0]))\n",
        "\n",
        "# Ask Groq model\n",
        "process_query(\"Summarize the key contributions of recent papers on machine learning\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3In4tWIU90oa",
        "outputId": "fda6dfb2-6611-42b0-a582-08cac8da1011"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results are saved in: papers/machine_learning/papers_info.json\n",
            "Paper IDs: ['1909.03550v1', '1811.04422v1', '1707.04849v1']\n",
            "{\n",
            "  \"title\": \"Lecture Notes: Optimization for Machine Learning\",\n",
            "  \"authors\": [\n",
            "    \"Elad Hazan\"\n",
            "  ],\n",
            "  \"summary\": \"Lecture notes on optimization for machine learning, derived from a course at\\nPrinceton University and tutorials given in MLSS, Buenos Aires, as well as\\nSimons Foundation, Berkeley.\",\n",
            "  \"pdf_url\": \"http://arxiv.org/pdf/1909.03550v1\",\n",
            "  \"published\": \"2019-09-08\"\n",
            "}\n",
            "What a monumental task! There have been countless papers on machine learning in recent years, and it's challenging to summarize the key contributions of all of them. However, I'll try to provide an overview of some influential papers in various areas of machine learning, highlighting their key contributions:\n",
            "\n",
            "**Deep Learning**\n",
            "\n",
            "1. **ResNet** (He et al., 2016): Introduced the concept of residual connections, which enable training of deeper neural networks and achieve state-of-the-art performance on image classification tasks.\n",
            "2. **BERT** (Devlin et al., 2019): Developed a language model that uses a multi-layer bidirectional transformer to achieve state-of-the-art results on a wide range of natural language processing tasks, including question answering, sentiment analysis, and text classification.\n",
            "3. **Transformers** (Vaswani et al., 2017): Introduced the transformer architecture, which has become a standard component in many state-of-the-art models for natural language processing and computer vision tasks.\n",
            "\n",
            "**Generative Models**\n",
            "\n",
            "1. **GANs** (Goodfellow et al., 2014): Introduced Generative Adversarial Networks, which have revolutionized the field of generative models and have been used for a wide range of applications, including image synthesis, data augmentation, and style transfer.\n",
            "2. **VAEs** (Kingma et al., 2014): Developed Variational Autoencoders, which have been used for unsupervised representation learning, dimensionality reduction, and generative modeling.\n",
            "\n",
            "**Reinforcement Learning**\n",
            "\n",
            "1. **DQN** (Mnih et al., 2015): Introduced Deep Q-Networks, which combined reinforcement learning with deep neural networks to achieve human-level performance on Atari games.\n",
            "2. **AlphaGo** (Silver et al., 2016): Developed a reinforcement learning algorithm that defeated a human world champion in Go, a complex board game.\n",
            "3. **IMPALA** (Espeholt et al., 2018): Introduced a scalable, distributed reinforcement learning algorithm that achieves state-of-the-art performance on a wide range of tasks.\n",
            "\n",
            "**Explainability and Interpretability**\n",
            "\n",
            "1. **LIME** (Ribeiro et al., 2016): Developed a technique for explaining the predictions of machine learning models, known as Local Interpretable Model-agnostic Explanations (LIME).\n",
            "2. **SHAP** (Lundberg et al., 2017): Introduced SHAP values, a technique for explaining the contribution of individual features to the predictions of machine learning models.\n",
            "\n",
            "**Robustness and Adversarial Attacks**\n",
            "\n",
            "1. **Adversarial Examples** (Szegedy et al., 2014): Introduced the concept of adversarial examples, which are inputs specifically designed to cause machine learning models to misbehave.\n",
            "2. **Defensive Distillation** (Papernot et al., 2016): Developed a technique for defending against adversarial attacks by distilling the knowledge of a teacher model into a student model.\n",
            "\n",
            "**Other Notable Papers**\n",
            "\n",
            "1. **Attention is All You Need** (Vaswani et al., 2017): Introduced the Transformer architecture, which relies solely on self-attention mechanisms to process input sequences.\n",
            "2. **DeepWalk** (Perozzi et al., 2014): Developed a technique for learning vertex representations in graphs, which has been widely used in graph neural networks.\n",
            "3. **Graph Convolutional Networks** (Kipf et al., 2017): Introduced graph convolutional networks, which have been used for a wide range of graph-based tasks, including node classification, graph classification, and link prediction.\n",
            "\n",
            "These papers represent just a small sample of the many influential contributions to the field of machine learning in recent years.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"What a monumental task! There have been countless papers on machine learning in recent years, and it's challenging to summarize the key contributions of all of them. However, I'll try to provide an overview of some influential papers in various areas of machine learning, highlighting their key contributions:\\n\\n**Deep Learning**\\n\\n1. **ResNet** (He et al., 2016): Introduced the concept of residual connections, which enable training of deeper neural networks and achieve state-of-the-art performance on image classification tasks.\\n2. **BERT** (Devlin et al., 2019): Developed a language model that uses a multi-layer bidirectional transformer to achieve state-of-the-art results on a wide range of natural language processing tasks, including question answering, sentiment analysis, and text classification.\\n3. **Transformers** (Vaswani et al., 2017): Introduced the transformer architecture, which has become a standard component in many state-of-the-art models for natural language processing and computer vision tasks.\\n\\n**Generative Models**\\n\\n1. **GANs** (Goodfellow et al., 2014): Introduced Generative Adversarial Networks, which have revolutionized the field of generative models and have been used for a wide range of applications, including image synthesis, data augmentation, and style transfer.\\n2. **VAEs** (Kingma et al., 2014): Developed Variational Autoencoders, which have been used for unsupervised representation learning, dimensionality reduction, and generative modeling.\\n\\n**Reinforcement Learning**\\n\\n1. **DQN** (Mnih et al., 2015): Introduced Deep Q-Networks, which combined reinforcement learning with deep neural networks to achieve human-level performance on Atari games.\\n2. **AlphaGo** (Silver et al., 2016): Developed a reinforcement learning algorithm that defeated a human world champion in Go, a complex board game.\\n3. **IMPALA** (Espeholt et al., 2018): Introduced a scalable, distributed reinforcement learning algorithm that achieves state-of-the-art performance on a wide range of tasks.\\n\\n**Explainability and Interpretability**\\n\\n1. **LIME** (Ribeiro et al., 2016): Developed a technique for explaining the predictions of machine learning models, known as Local Interpretable Model-agnostic Explanations (LIME).\\n2. **SHAP** (Lundberg et al., 2017): Introduced SHAP values, a technique for explaining the contribution of individual features to the predictions of machine learning models.\\n\\n**Robustness and Adversarial Attacks**\\n\\n1. **Adversarial Examples** (Szegedy et al., 2014): Introduced the concept of adversarial examples, which are inputs specifically designed to cause machine learning models to misbehave.\\n2. **Defensive Distillation** (Papernot et al., 2016): Developed a technique for defending against adversarial attacks by distilling the knowledge of a teacher model into a student model.\\n\\n**Other Notable Papers**\\n\\n1. **Attention is All You Need** (Vaswani et al., 2017): Introduced the Transformer architecture, which relies solely on self-attention mechanisms to process input sequences.\\n2. **DeepWalk** (Perozzi et al., 2014): Developed a technique for learning vertex representations in graphs, which has been widely used in graph neural networks.\\n3. **Graph Convolutional Networks** (Kipf et al., 2017): Introduced graph convolutional networks, which have been used for a wide range of graph-based tasks, including node classification, graph classification, and link prediction.\\n\\nThese papers represent just a small sample of the many influential contributions to the field of machine learning in recent years.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}